import math

import numpy as np
import pandas as pd
from numpy import where
from pylab import scatter, show, legend, xlabel, ylabel
from sklearn import preprocessing
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import LogisticRegression
from scipy import sparse

# scale values between -1,1 depending on the largest positive value in the data
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))
df = pd.read_csv("data_set_two.csv", header=0)

# clean/reset data
df.columns = ["g", "grade1", "grade2", "label"]
x = df["label"].map(lambda x: float(x))

# formats the input data into two arrays
X = df[["grade1", "grade2"]]
X = np.array(X)
X = min_max_scaler.fit_transform(X)
Y = df["label"].map(lambda x: float(x))
Y = np.array(Y)

# creating testing and training sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)

# train scikit learn model
clf = LogisticRegression()
clf.fit(X_train, Y_train)
print('score Scikit learn: ', clf.score(X_test, Y_test))

# visualize data using linear model
pos = where(Y == 1)
neg = where(Y == 0)
scatter(X[pos, 0], X[pos, 1], marker='o', c='b')
scatter(X[neg, 0], X[neg, 1], marker='x', c='r')
xlabel('Test 1 result')
ylabel('Test 2 result')
legend(['Not Admitted', 'Admitted'])
show()


# The sigmoid function adjusts the cost function hypotheses to adjust the algorithm proportionally..
def Sigmoid(z):
    g_f = float(1.0 / float((1.0 + math.exp(-1.0 * z))))
    return g_f


# This hypothesis will be used to calculate each instance of Cost Function
def Hypothesis(theta, x):
    z = 0
    for i in range(len(theta)):  # The hypothesis is the linear combination of all the known factors x[i] and their current estimated coefficients theta[i]
        z += x[i] * theta[i]
    return Sigmoid(z)


def Cost_Function(X, Y, theta, m):
    sumOfErrors = 0
    for i in range(m):  # For each member of the dataset, the result (Y) determines which variation of the cost
        # function is used
        xi = X[i]
        hi = Hypothesis(theta, xi)
        if Y[i] == 1:  # the Y = 1 it punishes low scores
            error = Y[i] * math.log(hi)
        elif Y[i] == 0:  # The Y = 0 cost function punishes high probability estimations..
            error = (1 - Y[i]) * math.log(1 - hi)
        sumOfErrors += error
    const = -1 / m
    J = const * sumOfErrors
    print('cost is ', J)
    return J


# This function creates the gradient component for each Theta value
def Cost_Function_Derivative(X, Y, theta, j, m, alpha):
    sumErrors = 0
    for i in range(m):#For each Theta there is a cost function calculated for each member of the dataset
        xi = X[i]
        xij = xi[j]
        hi = Hypothesis(theta,
                        X[i])  # The gradient is the partial derivative by Theta of the current value of theta minus
        error = (hi - Y[i]) * xij
        sumErrors += error
    m = len(Y)
    #a "learning speed factor aplha" times the average of all the cost functions for that theta
    constant = float(alpha) / float(m)
    J = constant * sumErrors
    return J

def Gradient_Descent(X, Y, theta, m, alpha):
    new_theta = []
    constant = alpha / m
    for j in range(len(theta)):#For each theta, the partial differential
        CFDerivative = Cost_Function_Derivative(X, Y, theta, j, m, alpha)
        new_theta_value = theta[j] - CFDerivative
        new_theta.append(new_theta_value)
    return new_theta


##set of values best representing the system in a linear combination model
def Logistic_Regression(X, Y, alpha, theta, num_iters):
    m = len(Y)
    for x in range(num_iters):
        new_theta = Gradient_Descent(X, Y, theta, m, alpha)
        theta = new_theta
        if x % 100 == 0:
            # here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration
            Cost_Function(X, Y, theta, m)
            print('theta ', theta)
            print('cost is ', Cost_Function(X, Y, theta, m))
    Declare_Winner(theta)


#This method compares the accuracy of the model generated by the scikit library
def Declare_Winner(theta):
    score = 0
    winner = ""
    scikit_score = clf.score(X_test, Y_test)
    length = len(X_test)
    for i in range(length):
        prediction = round(Hypothesis(X_test[i], theta))
        answer = Y_test[i]
        if prediction == answer:
            score += 1
    # the same process is repeated for the implementation
    my_score = float(score) / float(length)
    if my_score > scikit_score:
        print('You won!')
    elif my_score == scikit_score:
        print('Its a tie!')
    else:
        print('Your score: ', my_score)
    print('Scikits score: ', scikit_score)

# if the learning rate is too low that will not close on the most accurate values
# too high on alpha might overshoot the accurate values or cause irratic guesses
# Each iteration increases model accuracy but with diminishing returns, 
# and takes a signficicant coefficient times O(n)*|Theta|, n = dataset length
initial_theta = [0, 0]
alpha = 0.1
iterations = 1000
Logistic_Regression(X,Y,alpha,initial_theta,iterations)
